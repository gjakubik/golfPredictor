{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c84b5b1",
   "metadata": {},
   "source": [
    "# Golf Predictor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49306c98",
   "metadata": {},
   "source": [
    "## (0) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab42ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "# dfRaw is raw dataset read from csv\n",
    "dfRaw = pd.read_csv(\"../data/combined.csv\")\n",
    "distance_dict = {\"A\" : 1, \"B\" : 2, \"C\" : 3, \"D\" : 4, \"E\" : 5, \"F\" : 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b85121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rank_to_class(raw_ans):\n",
    "    ans = \"\"\n",
    "    if raw_ans < 21:\n",
    "        ans = \"A\"\n",
    "    elif raw_ans < 41:\n",
    "        ans = \"B\"\n",
    "    elif raw_ans < 61:\n",
    "        ans = \"C\"\n",
    "    elif raw_ans < 101:\n",
    "        ans = \"D\"\n",
    "    elif raw_ans < 201:\n",
    "        ans = \"E\"\n",
    "    else:\n",
    "        ans = \"F\"\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e790c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>owgr_points_current_avg</th>\n",
       "      <th>driving_dist_avg</th>\n",
       "      <th>top_tens</th>\n",
       "      <th>gir_pct</th>\n",
       "      <th>sg_p_avg</th>\n",
       "      <th>driving_acc_pct</th>\n",
       "      <th>scrambling_pct</th>\n",
       "      <th>adj_scoring_avg</th>\n",
       "      <th>sg_ott_avg</th>\n",
       "      <th>sg_apr_avg</th>\n",
       "      <th>sg_arg_avg</th>\n",
       "      <th>rounds</th>\n",
       "      <th>year</th>\n",
       "      <th>owgr_rank_year_plus_two</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Tiger Woods</td>\n",
       "      <td>13.22</td>\n",
       "      <td>293.2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>67.59</td>\n",
       "      <td>0.426</td>\n",
       "      <td>62.50</td>\n",
       "      <td>60.00</td>\n",
       "      <td>68.944</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>1.533</td>\n",
       "      <td>0.247</td>\n",
       "      <td>43</td>\n",
       "      <td>2013</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Adam Scott</td>\n",
       "      <td>9.25</td>\n",
       "      <td>297.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>68.80</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>61.84</td>\n",
       "      <td>56.38</td>\n",
       "      <td>69.300</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.055</td>\n",
       "      <td>44</td>\n",
       "      <td>2013</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Phil Mickelson</td>\n",
       "      <td>8.52</td>\n",
       "      <td>287.9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>66.67</td>\n",
       "      <td>0.661</td>\n",
       "      <td>57.30</td>\n",
       "      <td>58.55</td>\n",
       "      <td>69.742</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.260</td>\n",
       "      <td>57</td>\n",
       "      <td>2013</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Henrik Stenson</td>\n",
       "      <td>8.23</td>\n",
       "      <td>290.9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>71.96</td>\n",
       "      <td>0.004</td>\n",
       "      <td>70.09</td>\n",
       "      <td>57.28</td>\n",
       "      <td>69.248</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.128</td>\n",
       "      <td>44</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Justin Rose</td>\n",
       "      <td>7.78</td>\n",
       "      <td>296.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>68.89</td>\n",
       "      <td>-0.188</td>\n",
       "      <td>63.57</td>\n",
       "      <td>60.71</td>\n",
       "      <td>69.225</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.491</td>\n",
       "      <td>46</td>\n",
       "      <td>2013</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            name  owgr_points_current_avg  driving_dist_avg  \\\n",
       "0           0     Tiger Woods                    13.22             293.2   \n",
       "1           1      Adam Scott                     9.25             297.8   \n",
       "2           2  Phil Mickelson                     8.52             287.9   \n",
       "3           3  Henrik Stenson                     8.23             290.9   \n",
       "4           4     Justin Rose                     7.78             296.6   \n",
       "\n",
       "   top_tens  gir_pct  sg_p_avg  driving_acc_pct  scrambling_pct  \\\n",
       "0       8.0    67.59     0.426            62.50           60.00   \n",
       "1       6.0    68.80    -0.027            61.84           56.38   \n",
       "2       7.0    66.67     0.661            57.30           58.55   \n",
       "3       8.0    71.96     0.004            70.09           57.28   \n",
       "4       7.0    68.89    -0.188            63.57           60.71   \n",
       "\n",
       "   adj_scoring_avg  sg_ott_avg  sg_apr_avg  sg_arg_avg  rounds  year  \\\n",
       "0           68.944      -0.142       1.533       0.247      43  2013   \n",
       "1           69.300       0.734       0.548       0.055      44  2013   \n",
       "2           69.742       0.021       0.494       0.260      57  2013   \n",
       "3           69.248       0.710       0.776       0.128      44  2013   \n",
       "4           69.225       0.459       0.961       0.491      46  2013   \n",
       "\n",
       "   owgr_rank_year_plus_two  \n",
       "0                      299  \n",
       "1                       13  \n",
       "2                       24  \n",
       "3                        6  \n",
       "4                        7  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfConv = dfRaw\n",
    "\n",
    "# drop label \n",
    "dfConv = dfConv.drop([\"owgr_rank_year_plus_two\"], axis=1)\n",
    "dfConv[\"owgr_rank_year_plus_two\"] = dfRaw[\"owgr_rank_year_plus_two\"]\n",
    "\n",
    "# remove all ranks greater than 300\n",
    "for index, row in dfConv.iterrows():\n",
    "    \n",
    "    if row[\"top_tens\"] != row[\"top_tens\"]: # https://stackoverflow.com/questions/944700/how-can-i-check-for-nan-values?rq=1\n",
    "        dfConv.loc[index, \"top_tens\"] = 0.0\n",
    "    \n",
    "    # remove all ranks greater than 300\n",
    "    if row[\"owgr_rank_year_plus_two\"] > 300:\n",
    "        dfConv.drop(index, inplace=True) #https://stackoverflow.com/questions/28876243/how-to-delete-the-current-row-in-pandas-dataframe-during-df-iterrows\n",
    "    \n",
    "\n",
    "dfConv.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc3d5a",
   "metadata": {},
   "source": [
    "### (0.1) K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dab8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits()\n",
    "trainSets = []\n",
    "testSets = []\n",
    "for k in kf.split(dfConv):\n",
    "    trainSets.append(dfConv.iloc[k[0]])\n",
    "    testSets.append(dfConv.iloc[k[1]])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1a09d014",
   "metadata": {},
   "source": [
    "### (0.2) Find Feature Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c6d3336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['owgr_points_current_avg', 'driving_dist_avg', 'top_tens', 'gir_pct',\n",
       "       'sg_p_avg', 'driving_acc_pct', 'scrambling_pct', 'adj_scoring_avg',\n",
       "       'sg_ott_avg', 'sg_apr_avg', 'sg_arg_avg', 'rounds', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureNames = dfConv.columns[2:-1] # Skip ID, name, year, rounds, and rank as label\n",
    "nFeatures = featureNames.shape[0]\n",
    "print(\"Number of features:\", nFeatures)\n",
    "featureNames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab7faa",
   "metadata": {},
   "source": [
    "### (0.3) Select Training Instances (features and values) from the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a9e799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 1346\n",
      "Number of training instances: 1346\n",
      "Number of training instances: 1346\n",
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n"
     ]
    }
   ],
   "source": [
    "trainingInstancesList = []\n",
    "for train in trainSets:\n",
    "    trainingInstances_x = []\n",
    "    trainingInstances_y = []\n",
    "    for instance in train.to_numpy():\n",
    "        featureValues = list(instance[2:-1])\n",
    "        label = instance[-1]\n",
    "        trainingInstances_x.append(featureValues)\n",
    "        trainingInstances_y.append(label)\n",
    "    nTrainingInstances = len(trainingInstances_x)\n",
    "    trainingInstancesList.append((trainingInstances_x, trainingInstances_y, nTrainingInstances))\n",
    "    print(\"Number of training instances:\", nTrainingInstances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c11756",
   "metadata": {},
   "source": [
    "### (0.4) Select Test Instances from the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "535a8263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n"
     ]
    }
   ],
   "source": [
    "testInstancesList = []\n",
    "for test in testSets:    \n",
    "    testInstances_x = []\n",
    "    testInstances_y = []\n",
    "    for instance in test.to_numpy():\n",
    "        featureValues = list(instance[2:-1])\n",
    "        label = instance[-1]\n",
    "        testInstances_x.append(featureValues)\n",
    "        testInstances_y.append(label)\n",
    "    nTrainingInstances = len(trainingInstances_x)\n",
    "    print(\"Number of training instances:\", nTrainingInstances)\n",
    "    testInstancesList.append((testInstances_x, testInstances_y, nTrainingInstances))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82119c8c",
   "metadata": {},
   "source": [
    "### (0.5) Convert Rank to Class (except for Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "506b5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy # https://docs.python.org/3/library/copy.html\n",
    "trainingInstancesListRegression = copy.deepcopy(trainingInstancesList) # https://stackoverflow.com/questions/28684154/python-copy-a-list-of-lists\n",
    "for i, trainingInstances in enumerate(trainingInstancesList):\n",
    "    trainingInstancesListRegression.append(trainingInstances[:])\n",
    "    for j, rank in enumerate(trainingInstances[1]):\n",
    "        trainingInstancesList[i][1][j] = convert_rank_to_class(rank)\n",
    "\n",
    "# convert test instances final rank to a class\n",
    "for i, testInstances in enumerate(testInstancesList):\n",
    "    for j, rank in enumerate(testInstances[1]):\n",
    "        testInstancesList[i][1][j] = convert_rank_to_class(rank)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02193c",
   "metadata": {},
   "source": [
    "## 1. Neural Network with KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3325153f",
   "metadata": {},
   "source": [
    "### Uses Both Neural Network Regression and KNN Classification (5 Neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c90283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4124629080118694\n",
      "F1 Micro: 0.4124629080118694\n",
      "F1 Macro: 0.3326816861626153\n",
      "Accuracy: 0.31750741839762614\n",
      "F1 Micro: 0.31750741839762614\n",
      "F1 Macro: 0.24355785177577913\n",
      "Accuracy: 0.29376854599406527\n",
      "F1 Micro: 0.29376854599406527\n",
      "F1 Macro: 0.24648689646180444\n",
      "Accuracy: 0.36904761904761907\n",
      "F1 Micro: 0.36904761904761907\n",
      "F1 Macro: 0.327731029004133\n",
      "Accuracy: 0.3630952380952381\n",
      "F1 Micro: 0.3630952380952381\n",
      "F1 Macro: 0.36262730988553665\n",
      "Overall Precision:  [0.63981318 0.2907558  0.20041983 0.20932839 0.38013688 0.3510265 ]\n",
      "Overall Recall:  [0.45596725 0.21667219 0.11756272 0.20943471 0.52563729 0.31878402]\n",
      "Overall Accuracy:  0.3511763459092836\n",
      "Overall F1 Micro:  0.3511763459092836\n",
      "Overall F1 Macro:  0.30261695465797367\n",
      "Accuracy Stdev:  0.04649391696708961\n",
      "F1 Micro Stdev:  0.04649391696708961\n",
      "F1 Macro Stdev:  0.05425497353147425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor # https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler # https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n",
    "from sklearn import metrics\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "acc_stdev = []\n",
    "f1micro_stdev = []\n",
    "f1macro_stdev = []\n",
    "scaler1 = MinMaxScaler()\n",
    "scaler2 = MinMaxScaler()\n",
    "for trainingInstances, testInstances in zip(trainingInstancesListRegression, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    \n",
    "    trainingInstances_x_forNN = []\n",
    "    # drop the following cols \"rounds\", \"scrambling_pct\", \"year\", \"gir_pct\", \"sg_apr_avg\",\"sg_arg_avg\"\n",
    "    for i, inst in enumerate(trainingInstances_x):\n",
    "        trainingInstances_x_forNN.append([])\n",
    "        for j, val in enumerate(inst):\n",
    "            if j != 3 and j != 6 and j < 9:\n",
    "                trainingInstances_x_forNN[i].append(val)\n",
    "     \n",
    "    testInstances_x_forNN = []\n",
    "    # drop the following cols \"rounds\", \"scrambling_pct\", \"year\", \"gir_pct\", \"sg_apr_avg\",\"sg_arg_avg\"\n",
    "    for i, inst in enumerate(testInstances_x):\n",
    "        testInstances_x_forNN.append([])\n",
    "        for j, val in enumerate(inst):\n",
    "            if j != 3 and j != 6 and j < 9:\n",
    "                testInstances_x_forNN[i].append(val)\n",
    "            \n",
    "    \n",
    "    # use min max scaling on input\n",
    "    trainingInstancesScaled = scaler1.fit_transform(trainingInstances_x_forNN)\n",
    "    testInstancesScaled = scaler2.fit_transform(testInstances_x_forNN)\n",
    "    \n",
    "    # save training output as class for KNN\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    trainingInstances_y_WithClass = []\n",
    "    for val in trainingInstances_y:\n",
    "        trainingInstances_y_WithClass.append(convert_rank_to_class(val))\n",
    "         \n",
    "    # train KNN\n",
    "    clf_knn.fit(trainingInstances_x_forNN, trainingInstances_y_WithClass)\n",
    "    \n",
    "    # save testing output as class for KNN\n",
    "    testInstances_y_WithClass = []\n",
    "    for val in trainingInstances_y:\n",
    "        testInstances_y_WithClass.append(convert_rank_to_class(val))\n",
    "\n",
    "    # use KNN model to predict\n",
    "    y_pred2 = clf_knn.predict(testInstances_x_forNN)\n",
    "    \n",
    "    # train neural network and predict\n",
    "    clf_nn = MLPRegressor(max_iter=10000, random_state=0, hidden_layer_sizes=(7,))\n",
    "    clf_nn.fit(trainingInstancesScaled, trainingInstances_y)\n",
    "    y_pred_raw = clf_nn.predict(testInstancesScaled)\n",
    "    \n",
    "    # convert regression output to class output \n",
    "    y_pred = []\n",
    "    for i, val in enumerate(y_pred_raw):\n",
    "        nn_pred_val = convert_rank_to_class(val)\n",
    "        if y_pred2[i] == 'F': # if KNN said F, go with KNN's prediction\n",
    "            y_pred.append('F')\n",
    "        else:\n",
    "            y_pred.append(nn_pred_val)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    #print(\"Predicted:\", y_pred)\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    acc_stdev.append(metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    f1micro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    f1macro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    \n",
    "    \n",
    "print(\"Overall Precision: \", numpy.divide(prec, len(testSets)))\n",
    "print(\"Overall Recall: \", numpy.divide(recall, len(testSets)))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))\n",
    "\n",
    "print(\"Accuracy Stdev: \", numpy.std(acc_stdev, ddof=1))\n",
    "print(\"F1 Micro Stdev: \", numpy.std(f1micro_stdev, ddof=1))\n",
    "print(\"F1 Macro Stdev: \", numpy.std(f1macro_stdev, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9617d",
   "metadata": {},
   "source": [
    "## 2. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa542d3",
   "metadata": {},
   "source": [
    "### 2.1 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec69cbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.40059347181008903\n",
      "F1 Micro: 0.40059347181008903\n",
      "F1 Macro: 0.21605921527922448\n",
      "Accuracy: 0.34421364985163205\n",
      "F1 Micro: 0.34421364985163205\n",
      "F1 Macro: 0.19944331051296227\n",
      "Accuracy: 0.35311572700296734\n",
      "F1 Micro: 0.35311572700296734\n",
      "F1 Macro: 0.19428938167939547\n",
      "Accuracy: 0.38392857142857145\n",
      "F1 Micro: 0.38392857142857145\n",
      "F1 Macro: 0.23947829845932664\n",
      "Accuracy: 0.3630952380952381\n",
      "F1 Micro: 0.3630952380952381\n",
      "F1 Macro: 0.20898572000057147\n",
      "Overall MAE:  1.1354528755122226\n",
      "Overall Accuracy:  0.3689893316376996\n",
      "Overall F1 Micro:  0.3689893316376996\n",
      "Overall F1 Macro:  0.21165118518629605\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    \n",
    "    clf = LinearSVC(C=10, dual=False)\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None, zero_division=True))\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    #print(\"Predicted:\", y_pred)\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    y_pred_svm = y_pred\n",
    "    \n",
    "    \n",
    "#print(\"Overall Precision: \", numpy.divide(prec / [len(testSets), len(testSets), len(testSets), len(testSets), len(testSets), len(testSets)]))\n",
    "#print(\"Overall Recall: \", numpy.divide(recall / [len(testSets), len(testSets), len(testSets), len(testSets), len(testSets), len(testSets)]))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc319d",
   "metadata": {},
   "source": [
    "### 2.2 Kernel SVM -- Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "632a29af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3916913946587537\n",
      "F1 Micro: 0.3916913946587537\n",
      "F1 Macro: 0.24331275115107967\n",
      "Accuracy: 0.3264094955489614\n",
      "F1 Micro: 0.3264094955489614\n",
      "F1 Macro: 0.19976219273164378\n",
      "Accuracy: 0.3293768545994065\n",
      "F1 Micro: 0.3293768545994065\n",
      "F1 Macro: 0.21851833910623644\n",
      "Accuracy: 0.37797619047619047\n",
      "F1 Micro: 0.37797619047619047\n",
      "F1 Macro: 0.2647773907037842\n",
      "Accuracy: 0.3273809523809524\n",
      "F1 Micro: 0.3273809523809524\n",
      "F1 Macro: 0.18521631655709994\n",
      "Overall Precision:  [0.4648808  0.24264069 1.         0.09       0.35176741 0.30380952]\n",
      "Overall Recall:  [0.61747659 0.17410502 0.00666667 0.01230907 0.77433743 0.08407929]\n",
      "Overall MAE:  1.1866415854175496\n",
      "Overall Accuracy:  0.3505669775328529\n",
      "Overall F1 Micro:  0.3505669775328529\n",
      "Overall F1 Macro:  0.2223173980499688\n",
      "Accuracy Stdev:  0.03167285764735904\n",
      "F1 Micro Stdev:  0.03167285764735904\n",
      "F1 Macro Stdev:  0.03218429823591245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "acc_stdev = []\n",
    "f1micro_stdev = []\n",
    "f1macro_stdev = []\n",
    "scaler1=MinMaxScaler()\n",
    "scaler2=MinMaxScaler()\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    \n",
    "    trainingInstancesScaled = scaler1.fit_transform(trainingInstances_x)\n",
    "    testInstancesScaled = scaler2.fit_transform(testInstances_x)\n",
    "    \n",
    "    clf = SVC()\n",
    "    #clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    clf.fit(trainingInstancesScaled, trainingInstances_y)\n",
    "    #y_pred = clf.predict(testInstances_x)\n",
    "    y_pred = clf.predict(testInstancesScaled)\n",
    "    \n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None, zero_division=True))\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    \n",
    "    #print(\"Predicted:\", y_pred)\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    acc_stdev.append(metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    f1micro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    f1macro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    y_pred_svm = y_pred\n",
    "\n",
    "print(\"Overall Precision: \", numpy.divide(prec, len(testSets)))\n",
    "print(\"Overall Recall: \", numpy.divide(recall, len(testSets)))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))\n",
    "\n",
    "print(\"Accuracy Stdev: \", numpy.std(acc_stdev, ddof=1))\n",
    "print(\"F1 Micro Stdev: \", numpy.std(f1micro_stdev, ddof=1))\n",
    "print(\"F1 Macro Stdev: \", numpy.std(f1macro_stdev, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0702ed",
   "metadata": {},
   "source": [
    "### 2.3 Kernel SVM -- Quadratic Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b70f7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4094955489614243\n",
      "F1 Micro: 0.4094955489614243\n",
      "F1 Macro: 0.2632559466856599\n",
      "Accuracy: 0.32047477744807124\n",
      "F1 Micro: 0.32047477744807124\n",
      "F1 Macro: 0.1875882569046079\n",
      "Accuracy: 0.3293768545994065\n",
      "F1 Micro: 0.3293768545994065\n",
      "F1 Macro: 0.2363624849876966\n",
      "Accuracy: 0.34523809523809523\n",
      "F1 Micro: 0.34523809523809523\n",
      "F1 Macro: 0.29641932141932137\n",
      "Accuracy: 0.31547619047619047\n",
      "F1 Micro: 0.31547619047619047\n",
      "F1 Macro: 0.20362429851109423\n",
      "Overall Recall:  [0.55318476 0.30484174 0.02       0.04327643 0.66818668 0.14668175]\n",
      "Overall MAE:  1.1557739861523246\n",
      "Overall Accuracy:  0.34401229334463757\n",
      "Overall F1 Micro:  0.34401229334463757\n",
      "Overall F1 Macro:  0.237450061701676\n",
      "Accuracy Stdev:  0.038314746655891366\n",
      "F1 Micro Stdev:  0.038314746655891366\n",
      "F1 Macro Stdev:  0.0440876455132522\n"
     ]
    }
   ],
   "source": [
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "acc_stdev = []\n",
    "f1micro_stdev = []\n",
    "f1macro_stdev = []\n",
    "scaler1=MinMaxScaler()\n",
    "scaler2=MinMaxScaler()\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    \n",
    "    trainingInstancesScaled = scaler1.fit_transform(trainingInstances_x)\n",
    "    testInstancesScaled = scaler2.fit_transform(testInstances_x)\n",
    "    \n",
    "    clf = SVC(kernel='poly', degree=2)\n",
    "    #clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    clf.fit(trainingInstancesScaled, trainingInstances_y)\n",
    "    #y_pred = clf.predict(testInstances_x)\n",
    "    y_pred = clf.predict(testInstancesScaled)\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None, zero_division=True))\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    #print(\"Predicted:\", y_pred)\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    acc_stdev.append(metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    f1micro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    f1macro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    y_pred_svm = y_pred\n",
    "#print(\"Overall Precision: \", numpy.divide(prec, len(testSets)))\n",
    "print(\"Overall Recall: \", numpy.divide(recall, len(testSets)))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))\n",
    "\n",
    "print(\"Accuracy Stdev: \", numpy.std(acc_stdev, ddof=1))\n",
    "print(\"F1 Micro Stdev: \", numpy.std(f1micro_stdev, ddof=1))\n",
    "print(\"F1 Macro Stdev: \", numpy.std(f1macro_stdev, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f58e64",
   "metadata": {},
   "source": [
    "## 3. KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c7c8dc",
   "metadata": {},
   "source": [
    "### 3.1 KNN (with 5 neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91e1d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.29376854599406527\n",
      "F1 Micro: 0.29376854599406527\n",
      "F1 Macro: 0.23218674672180958\n",
      "Accuracy: 0.27299703264094954\n",
      "F1 Micro: 0.27299703264094954\n",
      "F1 Macro: 0.23231607365473636\n",
      "Accuracy: 0.28486646884273\n",
      "F1 Micro: 0.28486646884273\n",
      "F1 Macro: 0.25493801344420725\n",
      "Accuracy: 0.27976190476190477\n",
      "F1 Micro: 0.27976190476190477\n",
      "F1 Macro: 0.22758595671639148\n",
      "Accuracy: 0.2857142857142857\n",
      "F1 Micro: 0.2857142857142857\n",
      "F1 Macro: 0.2562070031003582\n",
      "Overall Recall:  [0.48426001 0.16098638 0.05464926 0.18418819 0.42102649 0.19784876]\n",
      "Overall MAE:  1.4433004804295606\n",
      "Overall Accuracy:  0.28342164759078703\n",
      "Overall F1 Micro:  0.28342164759078703\n",
      "Overall F1 Macro:  0.24064675872750058\n",
      "Accuracy Stdev:  0.007688763213434875\n",
      "F1 Micro Stdev:  0.007688763213434875\n",
      "F1 Macro Stdev:  0.013765153762419819\n"
     ]
    }
   ],
   "source": [
    "# KNN WITH n_neighbors=5\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "acc_stdev = []\n",
    "f1micro_stdev = []\n",
    "f1macro_stdev = []\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = KNeighborsClassifier() # n_neighbors=5\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    #print(\"Predicted:\", y_pred)\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    acc_stdev.append(metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    f1micro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    f1macro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "\n",
    "#print(\"Overall Precision: \", numpy.divide(prec, len(testSets)))\n",
    "print(\"Overall Recall: \", numpy.divide(recall, len(testSets)))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))\n",
    "\n",
    "print(\"Accuracy Stdev: \", numpy.std(acc_stdev, ddof=1))\n",
    "print(\"F1 Micro Stdev: \", numpy.std(f1micro_stdev, ddof=1))\n",
    "print(\"F1 Macro Stdev: \", numpy.std(f1macro_stdev, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a674a996",
   "metadata": {},
   "source": [
    "### 3.2 KNN (with 3 neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7093532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26112759643916916\n",
      "F1 Micro: 0.26112759643916916\n",
      "F1 Macro: 0.23112623308875543\n",
      "Accuracy: 0.22255192878338279\n",
      "F1 Micro: 0.22255192878338276\n",
      "F1 Macro: 0.20359842982944684\n",
      "Accuracy: 0.2344213649851632\n",
      "F1 Micro: 0.2344213649851632\n",
      "F1 Macro: 0.21010058122876377\n",
      "Accuracy: 0.26785714285714285\n",
      "F1 Micro: 0.26785714285714285\n",
      "F1 Macro: 0.23080318601055314\n",
      "Accuracy: 0.25892857142857145\n",
      "F1 Micro: 0.25892857142857145\n",
      "F1 Macro: 0.2418149164806831\n",
      "Overall Precision:  [0.3349094  0.14213983 0.08022887 0.20400299 0.35101328 0.23722462]\n",
      "Overall Recall:  [0.54721486 0.20422648 0.08292627 0.1887061  0.26838865 0.15498073]\n",
      "Overall MAE:  1.608398685883849\n",
      "Overall Accuracy:  0.2489773208986859\n",
      "Overall F1 Micro:  0.24897732089868585\n",
      "Overall F1 Macro:  0.22348866932764047\n",
      "Accuracy Stdev:  0.01945045925553679\n",
      "F1 Micro Stdev:  0.0194504592555368\n",
      "F1 Macro Stdev:  0.015988655348894206\n"
     ]
    }
   ],
   "source": [
    "# KNN WITH n_neighbors=3\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "acc_stdev = []\n",
    "f1micro_stdev = []\n",
    "f1macro_stdev = []\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = KNeighborsClassifier(n_neighbors=3) # n_neighbors = 3\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    #print(\"Predicted:\", y_pred)\n",
    "\n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    acc_stdev.append(metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    f1micro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    f1macro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "\n",
    "    \n",
    "print(\"Overall Precision: \", numpy.divide(prec, len(testSets)))\n",
    "print(\"Overall Recall: \", numpy.divide(recall, len(testSets)))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))\n",
    "\n",
    "print(\"Accuracy Stdev: \", numpy.std(acc_stdev, ddof=1))\n",
    "print(\"F1 Micro Stdev: \", numpy.std(f1micro_stdev, ddof=1))\n",
    "print(\"F1 Macro Stdev: \", numpy.std(f1macro_stdev, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b91f3",
   "metadata": {},
   "source": [
    "## 4. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1739869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.42136498516320475\n",
      "F1 Micro: 0.42136498516320475\n",
      "F1 Macro: 0.3368244106552645\n",
      "Accuracy: 0.36795252225519287\n",
      "F1 Micro: 0.36795252225519287\n",
      "F1 Macro: 0.31810963133788234\n",
      "Accuracy: 0.3264094955489614\n",
      "F1 Micro: 0.3264094955489614\n",
      "F1 Macro: 0.288230457980347\n",
      "Accuracy: 0.34226190476190477\n",
      "F1 Micro: 0.34226190476190477\n",
      "F1 Macro: 0.2914765751554731\n",
      "Accuracy: 0.39880952380952384\n",
      "F1 Micro: 0.39880952380952384\n",
      "F1 Macro: 0.32952812769192874\n",
      "Overall Precision:  [0.59338227 0.27223535 0.125      0.21027489 0.38193731 0.35335912]\n",
      "Overall Recall:  [0.56552414 0.29344927 0.01311828 0.15282949 0.51117747 0.42160046]\n",
      "Overall MAE:  1.0986134661579765\n",
      "Overall Accuracy:  0.37135968630775745\n",
      "Overall F1 Micro:  0.37135968630775745\n",
      "Overall F1 Macro:  0.3128338405641792\n",
      "Accuracy Stdev:  0.03915620257225982\n",
      "F1 Micro Stdev:  0.03915620257225982\n",
      "F1 Macro Stdev:  0.02204279850989457\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "acc_stdev = []\n",
    "f1micro_stdev = []\n",
    "f1macro_stdev = []\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    #print(\"Predicted:\"print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    acc_stdev.append(metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    f1micro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    f1macro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "\n",
    "\n",
    "print(\"Overall Precision: \", numpy.divide(prec, len(testSets)))\n",
    "print(\"Overall Recall: \", numpy.divide(recall, len(testSets)))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))\n",
    "\n",
    "print(\"Accuracy Stdev: \", numpy.std(acc_stdev, ddof=1))\n",
    "print(\"F1 Micro Stdev: \", numpy.std(f1micro_stdev, ddof=1))\n",
    "print(\"F1 Macro Stdev: \", numpy.std(f1macro_stdev, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e47bb",
   "metadata": {},
   "source": [
    "# 5. Naive Solution - Random Guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d15af68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.20771513353115728\n",
      "F1 Micro: 0.20771513353115728\n",
      "F1 Macro: 0.15748661080730106\n",
      "Accuracy: 0.21364985163204747\n",
      "F1 Micro: 0.21364985163204747\n",
      "F1 Macro: 0.15054875455406294\n",
      "Accuracy: 0.228486646884273\n",
      "F1 Micro: 0.228486646884273\n",
      "F1 Macro: 0.15568736135238598\n",
      "Accuracy: 0.2113095238095238\n",
      "F1 Micro: 0.2113095238095238\n",
      "F1 Macro: 0.14310542373012017\n",
      "Accuracy: 0.22321428571428573\n",
      "F1 Micro: 0.22321428571428573\n",
      "F1 Macro: 0.18117375698403423\n",
      "Overall Precision:  [0.10315288 0.11198944 0.06222673 0.18643716 0.29723761 0.20238295]\n",
      "Overall Recall:  [0.04696037 0.06862471 0.03772145 0.14470197 0.38476023 0.34669955]\n",
      "Overall MAE:  1.8299915218312843\n",
      "Overall Accuracy:  0.21687508831425747\n",
      "Overall F1 Micro:  0.21687508831425747\n",
      "Overall F1 Macro:  0.1576003814855809\n",
      "Accuracy Stdev:  0.008664527506484275\n",
      "F1 Micro Stdev:  0.008664527506484275\n",
      "F1 Macro Stdev:  0.014310886283719694\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "acc_stdev = []\n",
    "f1micro_stdev = []\n",
    "f1macro_stdev = []\n",
    "representative_sample = [\"A\", \"A\", \"B\", \"B\", \"C\", \"C\", \"D\", \"D\", \"D\", \"D\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\",\n",
    "                        \"E\", \"F\",\"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\"]\n",
    "\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    y_pred = [random.sample(representative_sample, 1)[0] for y in testInstances_y]\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "\n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None)) # https://www.educative.io/edpresso/how-to-add-one-array-to-another-array-in-python\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    acc_stdev.append(metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    f1micro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    f1macro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "\n",
    "print(\"Overall Precision: \", numpy.divide(prec, len(testSets)))\n",
    "print(\"Overall Recall: \", numpy.divide(recall, len(testSets)))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))\n",
    "\n",
    "print(\"Accuracy Stdev: \", numpy.std(acc_stdev, ddof=1))\n",
    "print(\"F1 Micro Stdev: \", numpy.std(f1micro_stdev, ddof=1))\n",
    "print(\"F1 Macro Stdev: \", numpy.std(f1macro_stdev, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a7ad51",
   "metadata": {},
   "source": [
    "# 6. Naive Solution - Use Last Year's Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d90fe219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1683\n",
      "1683\n",
      "Accuracy: 0.3196672608437314\n",
      "F1 Micro: 0.3196672608437314\n",
      "F1 Macro: 0.307310147545276\n",
      "Overall Precision:  [0.54612546 0.22943723 0.14754098 0.20437956 0.35159817 0.34965035]\n",
      "Overall Recall:  [0.59919028 0.265      0.15697674 0.20895522 0.32083333 0.3164557 ]\n",
      "Overall MAE:  1.1354723707664884\n",
      "Overall Accuracy:  0.3196672608437314\n",
      "Overall F1 Micro:  0.3196672608437314\n",
      "Overall F1 Macro:  0.307310147545276\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "\n",
    "main_data = pd.read_csv(\"../data/combined.csv\")\n",
    "df_continuity = pd.read_csv(\"../owgr_2004-2019.csv\")\n",
    "\n",
    "y_pred = []\n",
    "y_ans = []\n",
    "\n",
    "# run predictions from 2004 to 2019 (using players in main_data)\n",
    "for index, row in main_data.iterrows():\n",
    "            \n",
    "    if row[\"year\"] < 2004 or row[\"year\"] > 2019:\n",
    "        continue\n",
    "        \n",
    "    player = (df_continuity['name'] == row['name']) & (df_continuity['year'] == row['year'])\n",
    "    player_ranking_data = df_continuity.loc[player]\n",
    "\n",
    "    current_rank = player_ranking_data['current_rank'].values[0]\n",
    "    rank_year_plus_two = player_ranking_data['rank_year_plus_two'].values[0]\n",
    "\n",
    "    if rank_year_plus_two > 300:\n",
    "        continue\n",
    "\n",
    "    guess_instance = convert_rank_to_class(current_rank)\n",
    "    ans_instance = convert_rank_to_class(rank_year_plus_two)\n",
    "\n",
    "    y_pred.append(guess_instance)\n",
    "    y_ans.append(ans_instance)\n",
    "\n",
    "print(len(y_pred))\n",
    "print(len(y_ans))\n",
    "\n",
    "# CALCULATIONS FOR MAE\n",
    "y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "y_ans_as_distance = [distance_dict[a] for a in y_ans]\n",
    "MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "\n",
    "# Calculations for Precision and Recall\n",
    "prec = numpy.add(prec, metrics.precision_score(y_ans, y_pred, average=None)) # from https://stackoverflow.com/questions/45890328/sklearn-metrics-for-multiclass-classification\n",
    "recall = numpy.add(recall, metrics.recall_score(y_ans, y_pred, average=None))\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_ans, y_pred))\n",
    "acc_sum += metrics.accuracy_score(y_ans, y_pred)\n",
    "print(\"F1 Micro:\", metrics.f1_score(y_ans, y_pred, average='micro'))\n",
    "f1micro_sum += metrics.f1_score(y_ans, y_pred, average='micro')\n",
    "print(\"F1 Macro:\", metrics.f1_score(y_ans, y_pred, average='macro'))\n",
    "f1macro_sum += metrics.f1_score(y_ans, y_pred, average='macro')\n",
    "\n",
    "print(\"Overall Precision: \", prec/ 1)\n",
    "print(\"Overall Recall: \", recall/ 1)\n",
    "print(\"Overall MAE: \", MAE_sum / 1)\n",
    "print(\"Overall Accuracy: \", acc_sum / 1)\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / 1)\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
