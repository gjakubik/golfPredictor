{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c84b5b1",
   "metadata": {},
   "source": [
    "# Golf Predictor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49306c98",
   "metadata": {},
   "source": [
    "## (0) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab42ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "# dfRaw is raw dataset read from csv\n",
    "dfRaw = pd.read_csv(\"../data/combined.csv\")\n",
    "distance_dict = {\"A\" : 1, \"B\" : 2, \"C\" : 3, \"D\" : 4, \"E\" : 5, \"F\" : 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b85121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rank_to_class(raw_ans):\n",
    "    ans = \"\"\n",
    "    if raw_ans < 21:\n",
    "        ans = \"A\"\n",
    "    elif raw_ans < 41:\n",
    "        ans = \"B\"\n",
    "    elif raw_ans < 61:\n",
    "        ans = \"C\"\n",
    "    elif raw_ans < 101:\n",
    "        ans = \"D\"\n",
    "    elif raw_ans < 201:\n",
    "        ans = \"E\"\n",
    "    else:\n",
    "        ans = \"F\"\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e790c84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfConv = dfRaw\n",
    "\n",
    "# drop label \n",
    "dfConv = dfConv.drop([\"owgr_rank_year_plus_two\"], axis=1)\n",
    "dfConv[\"owgr_rank_year_plus_two\"] = dfRaw[\"owgr_rank_year_plus_two\"]\n",
    "\n",
    "# remove all ranks greater than 300\n",
    "for index, row in dfConv.iterrows():\n",
    "    \n",
    "    if row[\"top_tens\"] != row[\"top_tens\"]: # https://stackoverflow.com/questions/944700/how-can-i-check-for-nan-values?rq=1\n",
    "        dfConv.loc[index, \"top_tens\"] = 0.0\n",
    "    \n",
    "    # remove all ranks greater than 300 and convert ranks to classes\n",
    "    if row[\"owgr_rank_year_plus_two\"] > 300:\n",
    "        dfConv.drop(index, inplace=True) #https://stackoverflow.com/questions/28876243/how-to-delete-the-current-row-in-pandas-dataframe-during-df-iterrows\n",
    "    else:\n",
    "        dfConv.loc[index, \"owgr_rank_year_plus_two\"] = convert_rank_to_class(row[\"owgr_rank_year_plus_two\"]) # conver rank to class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc3d5a",
   "metadata": {},
   "source": [
    "### (0.1) K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dab8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits()\n",
    "trainSets = []\n",
    "testSets = []\n",
    "for k in kf.split(dfConv):\n",
    "    trainSets.append(dfConv.iloc[k[0]])\n",
    "    testSets.append(dfConv.iloc[k[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56851ddd",
   "metadata": {},
   "source": [
    "### (0.2) Find Feature Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c6d3336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['owgr_points_current_avg', 'driving_dist_avg', 'top_tens', 'gir_pct',\n",
       "       'sg_p_avg', 'driving_acc_pct', 'scrambling_pct', 'adj_scoring_avg',\n",
       "       'sg_ott_avg', 'sg_apr_avg', 'sg_arg_avg', 'rounds', 'year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureNames = dfConv.columns[2:-1] # Skip ID, name, year, rounds, and rank as label\n",
    "nFeatures = featureNames.shape[0]\n",
    "print(\"Number of features:\", nFeatures)\n",
    "featureNames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab7faa",
   "metadata": {},
   "source": [
    "### (0.3) Select Training Instances (features and values) from the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a9e799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 1346\n",
      "Number of training instances: 1346\n",
      "Number of training instances: 1346\n",
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n"
     ]
    }
   ],
   "source": [
    "trainingInstancesList = []\n",
    "for train in trainSets:\n",
    "    trainingInstances_x = []\n",
    "    trainingInstances_y = []\n",
    "    for instance in train.to_numpy():\n",
    "        featureValues = list(instance[2:-1])\n",
    "        label = instance[-1]\n",
    "        trainingInstances_x.append(featureValues)\n",
    "        trainingInstances_y.append(label)\n",
    "    nTrainingInstances = len(trainingInstances_x)\n",
    "    trainingInstancesList.append((trainingInstances_x, trainingInstances_y, nTrainingInstances))\n",
    "    print(\"Number of training instances:\", nTrainingInstances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c11756",
   "metadata": {},
   "source": [
    "### (0.4) Select Test Instances from the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "535a8263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n"
     ]
    }
   ],
   "source": [
    "testInstancesList = []\n",
    "for test in testSets:    \n",
    "    testInstances_x = []\n",
    "    testInstances_y = []\n",
    "    for instance in test.to_numpy():\n",
    "        featureValues = list(instance[2:-1])\n",
    "        label = instance[-1]\n",
    "        testInstances_x.append(featureValues)\n",
    "        testInstances_y.append(label)\n",
    "    nTrainingInstances = len(trainingInstances_x)\n",
    "    print(\"Number of training instances:\", nTrainingInstances)\n",
    "    testInstancesList.append((testInstances_x, testInstances_y, nTrainingInstances))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02193c",
   "metadata": {},
   "source": [
    "## 1. Support Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3325153f",
   "metadata": {},
   "source": [
    "### 1.1 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1c90283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.32047477744807124\n",
      "F1 Micro: 0.32047477744807124\n",
      "F1 Macro: 0.1645311978645312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.19584569732937684\n",
      "F1 Micro: 0.19584569732937684\n",
      "F1 Macro: 0.06828181565023671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.21364985163204747\n",
      "F1 Micro: 0.21364985163204747\n",
      "F1 Macro: 0.11500268384326356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.30952380952380953\n",
      "F1 Micro: 0.30952380952380953\n",
      "F1 Macro: 0.10097001763668433\n",
      "Accuracy: 0.25595238095238093\n",
      "F1 Micro: 0.25595238095238093\n",
      "F1 Macro: 0.1360506234878094\n",
      "Overall MAE:  1.669588102303236\n",
      "Overall Accuracy:  0.2590893033771372\n",
      "Overall F1 Micro:  0.2590893033771372\n",
      "Overall F1 Macro:  0.11696726769650505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n",
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = LinearSVC()\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    #print(\"Predicted:\", y_pred)\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    y_pred_svm = y_pred\n",
    "    \n",
    "    \n",
    "#print(\"Overall Precision: \", numpy.divide(prec / [len(testSets), len(testSets), len(testSets), len(testSets), len(testSets), len(testSets)]))\n",
    "#print(\"Overall Recall: \", numpy.divide(recall / [len(testSets), len(testSets), len(testSets), len(testSets), len(testSets), len(testSets)]))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9617d",
   "metadata": {},
   "source": [
    "### 1.2 Kernel SVM  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa542d3",
   "metadata": {},
   "source": [
    "Kernel is Radius Basis Function: Gaussian:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec69cbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2878338278931751\n",
      "F1 Micro: 0.2878338278931751\n",
      "F1 Macro: 0.07450076804915515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.27596439169139464\n",
      "F1 Micro: 0.27596439169139464\n",
      "F1 Macro: 0.07209302325581396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2878338278931751\n",
      "F1 Micro: 0.2878338278931751\n",
      "F1 Macro: 0.07450076804915515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2976190476190476\n",
      "F1 Micro: 0.2976190476190476\n",
      "F1 Macro: 0.0764525993883792\n",
      "Accuracy: 0.2767857142857143\n",
      "F1 Micro: 0.2767857142857143\n",
      "F1 Macro: 0.07226107226107226\n",
      "Overall Precision:  [0.         0.         0.         0.         0.28520736 0.        ]\n",
      "Overall Recall:  [0. 0. 0. 0. 1. 0.]\n",
      "Overall MAE:  1.4949095662003673\n",
      "Overall Accuracy:  0.2852073618765013\n",
      "Overall F1 Micro:  0.2852073618765013\n",
      "Overall F1 Macro:  0.07396164620071514\n",
      "Accuracy Stdev:  0.00900282228569729\n",
      "F1 Micro Stdev:  0.00900282228569729\n",
      "F1 Macro Stdev:  0.0018145148079078991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "acc_stdev = []\n",
    "f1micro_stdev = []\n",
    "f1macro_stdev = []\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = SVC()\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    \n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    \n",
    "    #print(\"Predicted:\", y_pred)\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    acc_stdev.append(metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    f1micro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    f1macro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    y_pred_svm = y_pred\n",
    "\n",
    "print(\"Overall Precision: \", numpy.divide(prec, len(testSets)))\n",
    "print(\"Overall Recall: \", numpy.divide(recall, len(testSets)))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))\n",
    "\n",
    "print(\"Accuracy Stdev: \", numpy.std(acc_stdev, ddof=1))\n",
    "print(\"F1 Micro Stdev: \", numpy.std(f1micro_stdev, ddof=1))\n",
    "print(\"F1 Macro Stdev: \", numpy.std(f1macro_stdev, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ab549",
   "metadata": {},
   "source": [
    "Kernel is Quadratic kernel (\"Degree-2 polynomial kernel\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcf609d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2878338278931751\n",
      "F1 Micro: 0.2878338278931751\n",
      "F1 Macro: 0.07450076804915515\n",
      "Accuracy: 0.27596439169139464\n",
      "F1 Micro: 0.27596439169139464\n",
      "F1 Macro: 0.07209302325581396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2878338278931751\n",
      "F1 Micro: 0.2878338278931751\n",
      "F1 Macro: 0.07450076804915515\n",
      "Accuracy: 0.2976190476190476\n",
      "F1 Micro: 0.2976190476190476\n",
      "F1 Macro: 0.0764525993883792\n",
      "Accuracy: 0.2767857142857143\n",
      "F1 Micro: 0.2767857142857143\n",
      "F1 Macro: 0.07226107226107226\n",
      "Overall Precision:  [0.         0.         0.         0.         0.28520736 0.        ]\n",
      "Overall Recall:  [0. 0. 0. 0. 1. 0.]\n",
      "Overall MAE:  1.4949095662003673\n",
      "Overall Accuracy:  0.2852073618765013\n",
      "Overall F1 Micro:  0.2852073618765013\n",
      "Overall F1 Macro:  0.07396164620071514\n",
      "Accuracy Stdev:  0.00900282228569729\n",
      "F1 Micro Stdev:  0.00900282228569729\n",
      "F1 Macro Stdev:  0.0018145148079078991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antoniodomel/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "acc_stdev = []\n",
    "f1micro_stdev = []\n",
    "f1macro_stdev = []\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = SVC(kernel='poly', degree=2)\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    #print(\"Predicted:\", y_pred)\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    acc_stdev.append(metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    f1micro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    f1macro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    y_pred_svm = y_pred\n",
    "print(\"Overall Precision: \", numpy.divide(prec, len(testSets)))\n",
    "print(\"Overall Recall: \", numpy.divide(recall, len(testSets)))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))\n",
    "\n",
    "print(\"Accuracy Stdev: \", numpy.std(acc_stdev, ddof=1))\n",
    "print(\"F1 Micro Stdev: \", numpy.std(f1micro_stdev, ddof=1))\n",
    "print(\"F1 Macro Stdev: \", numpy.std(f1macro_stdev, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f58e64",
   "metadata": {},
   "source": [
    "## 2. KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c7c8dc",
   "metadata": {},
   "source": [
    "### Training KNN Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91e1d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.29376854599406527\n",
      "F1 Micro: 0.29376854599406527\n",
      "F1 Macro: 0.23218674672180958\n",
      "Accuracy: 0.27299703264094954\n",
      "F1 Micro: 0.27299703264094954\n",
      "F1 Macro: 0.23231607365473636\n",
      "Accuracy: 0.28486646884273\n",
      "F1 Micro: 0.28486646884273\n",
      "F1 Macro: 0.25493801344420725\n",
      "Accuracy: 0.27976190476190477\n",
      "F1 Micro: 0.27976190476190477\n",
      "F1 Macro: 0.22758595671639148\n",
      "Accuracy: 0.2857142857142857\n",
      "F1 Micro: 0.2857142857142857\n",
      "F1 Macro: 0.2562070031003582\n",
      "Overall Precision:  [0.38454445 0.17476101 0.0774359  0.17815657 0.35116887 0.26940716]\n",
      "Overall Recall:  [0.48426001 0.16098638 0.05464926 0.18418819 0.42102649 0.19784876]\n",
      "Overall MAE:  1.4433004804295606\n",
      "Overall Accuracy:  0.28342164759078703\n",
      "Overall F1 Micro:  0.28342164759078703\n",
      "Overall F1 Macro:  0.24064675872750058\n",
      "Accuracy Stdev:  0.007688763213434875\n",
      "F1 Micro Stdev:  0.007688763213434875\n",
      "F1 Macro Stdev:  0.013765153762419819\n"
     ]
    }
   ],
   "source": [
    "# KNN WITH n_neighbors=5\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "acc_stdev = []\n",
    "f1micro_stdev = []\n",
    "f1macro_stdev = []\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = KNeighborsClassifier() # n_neighbors=5\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    #print(\"Predicted:\", y_pred)\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    acc_stdev.append(metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    f1micro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    f1macro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    y_pred_svm = y_pred\n",
    "print(\"Overall Precision: \", numpy.divide(prec, len(testSets)))\n",
    "print(\"Overall Recall: \", numpy.divide(recall, len(testSets)))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))\n",
    "\n",
    "print(\"Accuracy Stdev: \", numpy.std(acc_stdev, ddof=1))\n",
    "print(\"F1 Micro Stdev: \", numpy.std(f1micro_stdev, ddof=1))\n",
    "print(\"F1 Macro Stdev: \", numpy.std(f1macro_stdev, ddof=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7093532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.26112759643916916\n",
      "F1 Micro: 0.26112759643916916\n",
      "F1 Macro: 0.23112623308875543\n",
      "Accuracy: 0.22255192878338279\n",
      "F1 Micro: 0.22255192878338276\n",
      "F1 Macro: 0.20359842982944684\n",
      "Accuracy: 0.2344213649851632\n",
      "F1 Micro: 0.2344213649851632\n",
      "F1 Macro: 0.21010058122876377\n",
      "Accuracy: 0.26785714285714285\n",
      "F1 Micro: 0.26785714285714285\n",
      "F1 Macro: 0.23080318601055314\n",
      "Accuracy: 0.25892857142857145\n",
      "F1 Micro: 0.25892857142857145\n",
      "F1 Macro: 0.2418149164806831\n",
      "Overall Precision:  [0.3349094  0.14213983 0.08022887 0.20400299 0.35101328 0.23722462]\n",
      "Overall Recall:  [0.54721486 0.20422648 0.08292627 0.1887061  0.26838865 0.15498073]\n",
      "Overall MAE:  1.608398685883849\n",
      "Overall Accuracy:  0.2489773208986859\n",
      "Overall F1 Micro:  0.24897732089868585\n",
      "Overall F1 Macro:  0.22348866932764047\n",
      "Accuracy Stdev:  0.01945045925553679\n",
      "F1 Micro Stdev:  0.0194504592555368\n",
      "F1 Macro Stdev:  0.015988655348894206\n"
     ]
    }
   ],
   "source": [
    "# KNN WITH n_neighbors=3\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "acc_stdev = []\n",
    "f1micro_stdev = []\n",
    "f1macro_stdev = []\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = KNeighborsClassifier(n_neighbors=3) # n_neighbors = 3\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    #print(\"Predicted:\", y_pred)\n",
    "\n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    acc_stdev.append(metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    f1micro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    f1macro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    y_pred_svm = y_pred\n",
    "    \n",
    "print(\"Overall Precision: \", numpy.divide(prec, len(testSets)))\n",
    "print(\"Overall Recall: \", numpy.divide(recall, len(testSets)))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))\n",
    "\n",
    "print(\"Accuracy Stdev: \", numpy.std(acc_stdev, ddof=1))\n",
    "print(\"F1 Micro Stdev: \", numpy.std(f1micro_stdev, ddof=1))\n",
    "print(\"F1 Macro Stdev: \", numpy.std(f1macro_stdev, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b91f3",
   "metadata": {},
   "source": [
    "## 4. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1739869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.42136498516320475\n",
      "F1 Micro: 0.42136498516320475\n",
      "F1 Macro: 0.3368244106552645\n",
      "Accuracy: 0.36795252225519287\n",
      "F1 Micro: 0.36795252225519287\n",
      "F1 Macro: 0.31810963133788234\n",
      "Accuracy: 0.3264094955489614\n",
      "F1 Micro: 0.3264094955489614\n",
      "F1 Macro: 0.288230457980347\n",
      "Accuracy: 0.34226190476190477\n",
      "F1 Micro: 0.34226190476190477\n",
      "F1 Macro: 0.2914765751554731\n",
      "Accuracy: 0.39880952380952384\n",
      "F1 Micro: 0.39880952380952384\n",
      "F1 Macro: 0.32952812769192874\n",
      "Overall Precision:  [0.59338227 0.27223535 0.125      0.21027489 0.38193731 0.35335912]\n",
      "Overall Recall:  [0.56552414 0.29344927 0.01311828 0.15282949 0.51117747 0.42160046]\n",
      "Overall MAE:  1.0986134661579765\n",
      "Overall Accuracy:  0.37135968630775745\n",
      "Overall F1 Micro:  0.37135968630775745\n",
      "Overall F1 Macro:  0.3128338405641792\n",
      "Accuracy Stdev:  0.03915620257225982\n",
      "F1 Micro Stdev:  0.03915620257225982\n",
      "F1 Macro Stdev:  0.02204279850989457\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "acc_stdev = []\n",
    "f1micro_stdev = []\n",
    "f1macro_stdev = []\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    #print(\"Predicted:\"print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    acc_stdev.append(metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    f1micro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    f1macro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    y_pred_svm = y_pred\n",
    "\n",
    "print(\"Overall Precision: \", numpy.divide(prec, len(testSets)))\n",
    "print(\"Overall Recall: \", numpy.divide(recall, len(testSets)))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))\n",
    "\n",
    "print(\"Accuracy Stdev: \", numpy.std(acc_stdev, ddof=1))\n",
    "print(\"F1 Micro Stdev: \", numpy.std(f1micro_stdev, ddof=1))\n",
    "print(\"F1 Macro Stdev: \", numpy.std(f1macro_stdev, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e47bb",
   "metadata": {},
   "source": [
    "# 5. Naive Solution - Random Guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d15af68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.17210682492581603\n",
      "F1 Micro: 0.17210682492581603\n",
      "F1 Macro: 0.1455795701199647\n",
      "Accuracy: 0.2195845697329377\n",
      "F1 Micro: 0.2195845697329377\n",
      "F1 Macro: 0.15909187810580952\n",
      "Accuracy: 0.18397626112759644\n",
      "F1 Micro: 0.18397626112759644\n",
      "F1 Macro: 0.13629870295548277\n",
      "Accuracy: 0.17261904761904762\n",
      "F1 Micro: 0.17261904761904762\n",
      "F1 Macro: 0.1293027758607469\n",
      "Accuracy: 0.18154761904761904\n",
      "F1 Micro: 0.18154761904761904\n",
      "F1 Macro: 0.13965868784559932\n",
      "Overall Precision:  [0.21500733 0.08969714 0.05781513 0.12033726 0.24852813 0.18453173]\n",
      "Overall Recall:  [0.08680292 0.050265   0.03401434 0.11890283 0.2941951  0.32215033]\n",
      "Overall MAE:  1.864483538222411\n",
      "Overall Accuracy:  0.18596686449060335\n",
      "Overall F1 Micro:  0.18596686449060335\n",
      "Overall F1 Macro:  0.14198632297752062\n",
      "Accuracy Stdev:  0.01951862731961884\n",
      "F1 Micro Stdev:  0.01951862731961884\n",
      "F1 Macro Stdev:  0.011226650386017554\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "acc_stdev = []\n",
    "f1micro_stdev = []\n",
    "f1macro_stdev = []\n",
    "representative_sample = [\"A\", \"A\", \"B\", \"B\", \"C\", \"C\", \"D\", \"D\", \"D\", \"D\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\",\n",
    "                        \"E\", \"F\",\"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\"]\n",
    "\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    y_pred = [random.sample(representative_sample, 1)[0] for y in testInstances_y]\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "\n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(testInstances_y, y_pred, average=None)) # https://www.educative.io/edpresso/how-to-add-one-array-to-another-array-in-python\n",
    "    recall = numpy.add(recall, metrics.recall_score(testInstances_y, y_pred, average=None))\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    acc_stdev.append(metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    f1micro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    f1macro_stdev.append(metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    y_pred_svm = y_pred\n",
    "print(\"Overall Precision: \", numpy.divide(prec, len(testSets)))\n",
    "print(\"Overall Recall: \", numpy.divide(recall, len(testSets)))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))\n",
    "\n",
    "print(\"Accuracy Stdev: \", numpy.std(acc_stdev, ddof=1))\n",
    "print(\"F1 Micro Stdev: \", numpy.std(f1micro_stdev, ddof=1))\n",
    "print(\"F1 Macro Stdev: \", numpy.std(f1macro_stdev, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a7ad51",
   "metadata": {},
   "source": [
    "# 6. Naive Solution - Use Last Year's Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d90fe219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1683\n",
      "1683\n",
      "Accuracy: 0.3196672608437314\n",
      "F1 Micro: 0.3196672608437314\n",
      "F1 Macro: 0.307310147545276\n",
      "Overall Precision:  [0.54612546 0.22943723 0.14754098 0.20437956 0.35159817 0.34965035]\n",
      "Overall Recall:  [0.59919028 0.265      0.15697674 0.20895522 0.32083333 0.3164557 ]\n",
      "Overall MAE:  1.1354723707664884\n",
      "Overall Accuracy:  0.3196672608437314\n",
      "Overall F1 Micro:  0.3196672608437314\n",
      "Overall F1 Macro:  0.307310147545276\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "\n",
    "main_data = pd.read_csv(\"../data/combined.csv\")\n",
    "df_continuity = pd.read_csv(\"../owgr_2004-2019.csv\")\n",
    "\n",
    "y_pred = []\n",
    "y_ans = []\n",
    "\n",
    "# run predictions from 2004 to 2019 (using players in main_data)\n",
    "for index, row in main_data.iterrows():\n",
    "            \n",
    "    if row[\"year\"] < 2004 or row[\"year\"] > 2019:\n",
    "        continue\n",
    "        \n",
    "    player = (df_continuity['name'] == row['name']) & (df_continuity['year'] == row['year'])\n",
    "    player_ranking_data = df_continuity.loc[player]\n",
    "\n",
    "    current_rank = player_ranking_data['current_rank'].values[0]\n",
    "    rank_year_plus_two = player_ranking_data['rank_year_plus_two'].values[0]\n",
    "\n",
    "    if rank_year_plus_two > 300:\n",
    "        continue\n",
    "\n",
    "    guess_instance = convert_rank_to_class(current_rank)\n",
    "    ans_instance = convert_rank_to_class(rank_year_plus_two)\n",
    "\n",
    "    y_pred.append(guess_instance)\n",
    "    y_ans.append(ans_instance)\n",
    "\n",
    "print(len(y_pred))\n",
    "print(len(y_ans))\n",
    "\n",
    "# CALCULATIONS FOR MAE\n",
    "y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "y_ans_as_distance = [distance_dict[a] for a in y_ans]\n",
    "MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "\n",
    "# Calculations for Precision and Recall\n",
    "prec = numpy.add(prec, metrics.precision_score(y_ans, y_pred, average=None)) # from https://stackoverflow.com/questions/45890328/sklearn-metrics-for-multiclass-classification\n",
    "recall = numpy.add(recall, metrics.recall_score(y_ans, y_pred, average=None))\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_ans, y_pred))\n",
    "acc_sum += metrics.accuracy_score(y_ans, y_pred)\n",
    "print(\"F1 Micro:\", metrics.f1_score(y_ans, y_pred, average='micro'))\n",
    "f1micro_sum += metrics.f1_score(y_ans, y_pred, average='micro')\n",
    "print(\"F1 Macro:\", metrics.f1_score(y_ans, y_pred, average='macro'))\n",
    "f1macro_sum += metrics.f1_score(y_ans, y_pred, average='macro')\n",
    "\n",
    "print(\"Overall Precision: \", prec/ 1)\n",
    "print(\"Overall Recall: \", recall/ 1)\n",
    "print(\"Overall MAE: \", MAE_sum / 1)\n",
    "print(\"Overall Accuracy: \", acc_sum / 1)\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / 1)\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805dac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
