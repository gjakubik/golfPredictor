{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c84b5b1",
   "metadata": {},
   "source": [
    "# Golf Predictor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49306c98",
   "metadata": {},
   "source": [
    "## (0) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab42ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# dfRaw is raw dataset read from csv\n",
    "dfRaw = pd.read_csv(\"../data/combined.csv\")\n",
    "distance_dict = {\"A\" : 1, \"B\" : 2, \"C\" : 3, \"D\" : 4, \"E\" : 5, \"F\" : 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "144675ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rank_to_class(raw_ans):\n",
    "    ans = \"\"\n",
    "    if raw_ans < 21:\n",
    "        ans = \"A\"\n",
    "    elif raw_ans < 41:\n",
    "        ans = \"B\"\n",
    "    elif raw_ans < 61:\n",
    "        ans = \"C\"\n",
    "    elif raw_ans < 101:\n",
    "        ans = \"D\"\n",
    "    elif raw_ans < 201:\n",
    "        ans = \"E\"\n",
    "    else:\n",
    "        ans = \"F\"\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc69289e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0            name  owgr_points_current_avg  driving_dist_avg  \\\n",
      "0              0     Tiger Woods                    13.22             293.2   \n",
      "1              1      Adam Scott                     9.25             297.8   \n",
      "2              2  Phil Mickelson                     8.52             287.9   \n",
      "3              3  Henrik Stenson                     8.23             290.9   \n",
      "4              4     Justin Rose                     7.78             296.6   \n",
      "...          ...             ...                      ...               ...   \n",
      "2364        2364     Matt Kuchar                     0.45             287.3   \n",
      "2370        2370    Jason Dufner                     0.33             290.7   \n",
      "2373        2373     Chris Couch                     0.27             302.1   \n",
      "2374        2374     Omar Uresti                     0.27             272.2   \n",
      "2375        2375        Ken Duke                     0.17             284.3   \n",
      "\n",
      "      top_tens  gir_pct  sg_p_avg  driving_acc_pct  scrambling_pct  \\\n",
      "0          8.0    67.59     0.426            62.50           60.00   \n",
      "1          6.0    68.80    -0.027            61.84           56.38   \n",
      "2          7.0    66.67     0.661            57.30           58.55   \n",
      "3          8.0    71.96     0.004            70.09           57.28   \n",
      "4          7.0    68.89    -0.188            63.57           60.71   \n",
      "...        ...      ...       ...              ...             ...   \n",
      "2364       1.0    65.33     0.029            64.35           58.11   \n",
      "2370       1.0    66.17    -0.074            62.49           55.51   \n",
      "2373       1.0    60.81    -0.104            56.21           42.78   \n",
      "2374       1.0    62.61    -0.438            73.13           63.81   \n",
      "2375       1.0    61.88    -0.051            59.01           54.27   \n",
      "\n",
      "      adj_scoring_avg  sg_ott_avg  sg_apr_avg  sg_arg_avg  rounds  year  \n",
      "0              68.944      -0.142       1.533       0.247      43  2013  \n",
      "1              69.300       0.734       0.548       0.055      44  2013  \n",
      "2              69.742       0.021       0.494       0.260      57  2013  \n",
      "3              69.248       0.710       0.776       0.128      44  2013  \n",
      "4              69.225       0.459       0.961       0.491      46  2013  \n",
      "...               ...         ...         ...         ...     ...   ...  \n",
      "2364           71.574       0.059      -0.282       0.091      77  2004  \n",
      "2370           71.339       0.083      -0.087       0.224      72  2004  \n",
      "2373           73.464       0.224      -1.088      -1.269      46  2004  \n",
      "2374           71.705      -0.194       0.147       0.221      76  2004  \n",
      "2375           72.247      -0.404      -0.488      -0.134      80  2004  \n",
      "\n",
      "[1683 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "dfConv = dfRaw\n",
    "\n",
    "# remove all ranks greater than 300\n",
    "for index, row in dfConv.iterrows():\n",
    "    if row[\"owgr_rank_year_plus_two\"] > 300:\n",
    "        dfConv.drop(index, inplace=True) #https://stackoverflow.com/questions/28876243/how-to-delete-the-current-row-in-pandas-dataframe-during-df-iterrows\n",
    "    else:\n",
    "        dfConv.loc[index, \"owgr_rank_year_plus_two\"] = convert_rank_to_class(row[\"owgr_rank_year_plus_two\"]) # conver rank to class\n",
    "\n",
    "# drop label \n",
    "dfConv = dfConv.drop([\"owgr_rank_year_plus_two\"], axis=1)\n",
    "print(dfConv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccc3d5a",
   "metadata": {},
   "source": [
    "### (0.1) K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dab8be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits()\n",
    "trainSets = []\n",
    "testSets = []\n",
    "for k in kf.split(dfRaw):\n",
    "    trainSets.append(dfConv.iloc[k[0]])\n",
    "    testSets.append(dfConv.iloc[k[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56851ddd",
   "metadata": {},
   "source": [
    "### (0.2) Find Feature Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c6d3336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['driving_dist_avg', 'top_tens', 'gir_pct', 'sg_p_avg',\n",
       "       'driving_acc_pct', 'scrambling_pct', 'adj_scoring_avg', 'sg_ott_avg',\n",
       "       'sg_apr_avg', 'sg_arg_avg', 'rounds'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureNames = dfConv.columns[3:-1] # Skip ID, name, year, rounds, and rank as label\n",
    "nFeatures = featureNames.shape[0]\n",
    "print(\"Number of features:\", nFeatures)\n",
    "featureNames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fab7faa",
   "metadata": {},
   "source": [
    "### (0.3) Select Training Instances (features and values) from the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a9e799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 1346\n",
      "Number of training instances: 1346\n",
      "Number of training instances: 1346\n",
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n"
     ]
    }
   ],
   "source": [
    "trainingInstancesList = []\n",
    "for train in trainSets:\n",
    "    trainingInstances_x = []\n",
    "    trainingInstances_y = []\n",
    "    for instance in train.to_numpy():\n",
    "        featureValues = list(instance[3:-1])\n",
    "        label = instance[-1]\n",
    "        trainingInstances_x.append(featureValues)\n",
    "        trainingInstances_y.append(label)\n",
    "    nTrainingInstances = len(trainingInstances_x)\n",
    "    trainingInstancesList.append((trainingInstances_x, trainingInstances_y, nTrainingInstances))\n",
    "    print(\"Number of training instances:\", nTrainingInstances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c11756",
   "metadata": {},
   "source": [
    "### (0.4) Select Test Instances from the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "535a8263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n",
      "Number of training instances: 1347\n"
     ]
    }
   ],
   "source": [
    "testInstancesList = []\n",
    "for test in testSets:    \n",
    "    testInstances_x = []\n",
    "    testInstances_y = []\n",
    "    for instance in test.to_numpy():\n",
    "        featureValues = list(instance[3:-1])\n",
    "        label = instance[-1]\n",
    "        testInstances_x.append(featureValues)\n",
    "        testInstances_y.append(label)\n",
    "    nTrainingInstances = len(trainingInstances_x)\n",
    "    print(\"Number of training instances:\", nTrainingInstances)\n",
    "    testInstancesList.append((testInstances_x, testInstances_y, nTrainingInstances))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f02193c",
   "metadata": {},
   "source": [
    "## 1. Support Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3325153f",
   "metadata": {},
   "source": [
    "### 1.1 Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1c90283",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-03a9cee68b72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtestInstances_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestInstances\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingInstances_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainingInstances_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestInstances_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    226\u001b[0m                              % self.C)\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         X, y = self._validate_data(X, y, accept_sparse='csr',\n\u001b[0m\u001b[1;32m    229\u001b[0m                                    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                                    accept_large_sparse=False)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y cannot be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[1;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 663\u001b[0;31m             _assert_all_finite(array,\n\u001b[0m\u001b[1;32m    664\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m    102\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    104\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     (type_err,\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = LinearSVC()\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(y_ans, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(y_ans, y_pred, average=None))\n",
    "    \n",
    "    #print(\"Predicted:\", y_pred)\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    y_pred_svm = y_pred\n",
    "print(\"Overall Precision: \", prec / len(testSets))\n",
    "print(\"Overall Recall: \", recall / len(testSets))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe9617d",
   "metadata": {},
   "source": [
    "### 1.2 Kernel SVM  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa542d3",
   "metadata": {},
   "source": [
    "Kernel is Radius Basis Function: Gaussian:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec69cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = SVC()\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    \n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(y_ans, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(y_ans, y_pred, average=None))\n",
    "    \n",
    "    \n",
    "    #print(\"Predicted:\", y_pred)\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    y_pred_svm = y_pred\n",
    "    \n",
    "print(\"Overall Precision: \", prec / len(testSets))\n",
    "print(\"Overall Recall: \", recall / len(testSets))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8ab549",
   "metadata": {},
   "source": [
    "Kernel is Quadratic kernel (\"Degree-2 polynomial kernel\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf609d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = SVC(kernel='poly', degree=2)\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(y_ans, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(y_ans, y_pred, average=None))\n",
    "    \n",
    "    #print(\"Predicted:\", y_pred)\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    y_pred_svm = y_pred\n",
    "print(\"Overall Precision: \", prec / len(testSets))\n",
    "print(\"Overall Recall: \", recall / len(testSets))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f58e64",
   "metadata": {},
   "source": [
    "## 2. KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c7c8dc",
   "metadata": {},
   "source": [
    "### Training KNN Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN WITH n_neighbors=5\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = KNeighborsClassifier() # n_neighbors=5\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    #print(\"Predicted:\", y_pred)\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(y_ans, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(y_ans, y_pred, average=None))\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    y_pred_svm = y_pred\n",
    "print(\"Overall Precision: \", prec / len(testSets))\n",
    "print(\"Overall Recall: \", recall / len(testSets))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7093532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN WITH n_neighbors=3\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = KNeighborsClassifier(n_neighbors=3) # n_neighbors = 3\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    #print(\"Predicted:\", y_pred)\n",
    "\n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(y_ans, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(y_ans, y_pred, average=None))\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    y_pred_svm = y_pred\n",
    "    \n",
    "print(\"Overall Precision: \", prec / len(testSets))\n",
    "print(\"Overall Recall: \", recall / len(testSets))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409b91f3",
   "metadata": {},
   "source": [
    "## 4. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1739869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(trainingInstances_x, trainingInstances_y)\n",
    "    y_pred = clf.predict(testInstances_x)\n",
    "    #print(\"Predicted:\"print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(y_ans, y_pred, average=None))\n",
    "    recall = numpy.add(recall, metrics.recall_score(y_ans, y_pred, average=None))\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    y_pred_svm = y_pred\n",
    "    \n",
    "print(\"Overall Precision: \", prec/ len(testSets))\n",
    "print(\"Overall Recall: \", recall/ len(testSets))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e47bb",
   "metadata": {},
   "source": [
    "# 5. Naive Solution - Random Guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15af68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "prec = [0,0,0,0,0,0]\n",
    "recall = [0,0,0,0,0,0]\n",
    "representative_sample = [\"A\", \"A\", \"B\", \"B\", \"C\", \"C\", \"D\", \"D\", \"D\", \"D\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\", \"E\",\n",
    "                        \"E\", \"F\",\"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\"]\n",
    "\n",
    "for trainingInstances, testInstances in zip(trainingInstancesList, testInstancesList):\n",
    "    trainingInstances_x = trainingInstances[0]\n",
    "    trainingInstances_y = trainingInstances[1]\n",
    "    testInstances_x = testInstances[0]\n",
    "    testInstances_y = testInstances[1]\n",
    "    y_pred = [random.sample(representative_sample, 1)[0] for y in testInstances_y]\n",
    "    \n",
    "    # CALCULATIONS FOR MAE\n",
    "    y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "    y_ans_as_distance = [distance_dict[a] for a in testInstances_y]\n",
    "    MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "    \n",
    "    # Calculations for Precision and Recall\n",
    "    prec = numpy.add(prec, metrics.precision_score(y_ans, y_pred, average=None)) # https://www.educative.io/edpresso/how-to-add-one-array-to-another-array-in-python\n",
    "    recall = numpy.add(recall, metrics.recall_score(y_ans, y_pred, average=None))\n",
    "    \n",
    "    print(\"Accuracy:\", metrics.accuracy_score(testInstances_y, y_pred))\n",
    "    acc_sum += metrics.accuracy_score(testInstances_y, y_pred)\n",
    "    print(\"F1 Micro:\", metrics.f1_score(testInstances_y, y_pred, average='micro'))\n",
    "    f1micro_sum += metrics.f1_score(testInstances_y, y_pred, average='micro')\n",
    "    print(\"F1 Macro:\", metrics.f1_score(testInstances_y, y_pred, average='macro'))\n",
    "    f1macro_sum += metrics.f1_score(testInstances_y, y_pred, average='macro')\n",
    "    y_pred_svm = y_pred\n",
    "print(\"Overall Precision: \", prec/ len(testSets))\n",
    "print(\"Overall Recall: \", recall/ len(testSets))\n",
    "print(\"Overall MAE: \", MAE_sum / len(testSets))\n",
    "print(\"Overall Accuracy: \", acc_sum / len(testSets))\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / len(testSets))\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / len(testSets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a7ad51",
   "metadata": {},
   "source": [
    "# 6. Naive Solution - Use Last Year's Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90fe219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "acc_sum = 0\n",
    "f1micro_sum = 0\n",
    "f1macro_sum = 0\n",
    "MAE_sum = 0\n",
    "\n",
    "\n",
    "main_data = pd.read_csv(\"../data/combined.csv\")\n",
    "df_continuity = pd.read_csv(\"../owgr_2004-2019.csv\")\n",
    "\n",
    "y_pred = []\n",
    "y_ans = []\n",
    "\n",
    "# run predictions from 2004 to 2019 (using players in main_data)\n",
    "for index, row in main_data[main_data['year'] >= 2004 & main_data['year'] <= 2019].iterrows():\n",
    "    player = (df_continuity['name'] == row['name']) & (df_continuity['year'] == row['year'])\n",
    "    player_ranking_data = df_continuity.loc[player]\n",
    "\n",
    "    current_rank = player_ranking_data['current_rank'].values[0]\n",
    "    rank_year_plus_two = player_ranking_data['rank_year_plus_two'].values[0]\n",
    "\n",
    "    if rank_year_plus_two > 300:\n",
    "        continue\n",
    "\n",
    "    guess_instance = convert_rank_to_class(current_rank)\n",
    "    ans_instance = convert_rank_to_class(rank_year_plus_two)\n",
    "\n",
    "    y_pred.append(guess_instance)\n",
    "    y_ans.append(ans_instance)\n",
    "\n",
    "print(len(y_pred))\n",
    "print(len(y_ans))\n",
    "\n",
    "# CALCULATIONS FOR MAE\n",
    "y_pred_as_distance = [distance_dict[a] for a in y_pred]\n",
    "y_ans_as_distance = [distance_dict[a] for a in y_ans]\n",
    "MAE_sum += metrics.mean_absolute_error(y_ans_as_distance, y_pred_as_distance)\n",
    "\n",
    "# Calculations for Precision and Recall\n",
    "prec = numpy.add(prec, metrics.precision_score(y_ans, y_pred, average=None)) # from https://stackoverflow.com/questions/45890328/sklearn-metrics-for-multiclass-classification\n",
    "recall = numpy.add(recall, metrics.recall_score(y_ans, y_pred, average=None))\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_ans, y_pred))\n",
    "acc_sum += metrics.accuracy_score(y_ans, y_pred)\n",
    "print(\"F1 Micro:\", metrics.f1_score(y_ans, y_pred, average='micro'))\n",
    "f1micro_sum += metrics.f1_score(y_ans, y_pred, average='micro')\n",
    "print(\"F1 Macro:\", metrics.f1_score(y_ans, y_pred, average='macro'))\n",
    "f1macro_sum += metrics.f1_score(y_ans, y_pred, average='macro')\n",
    "\n",
    "print(\"Overall Precision: \", prec/ 1)\n",
    "print(\"Overall Recall: \", recall/ 1\n",
    "print(\"Overall MAE: \", MAE_sum / 1)\n",
    "print(\"Overall Accuracy: \", acc_sum / 1)\n",
    "print(\"Overall F1 Micro: \", f1micro_sum / 1)\n",
    "print(\"Overall F1 Macro: \", f1macro_sum / 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
